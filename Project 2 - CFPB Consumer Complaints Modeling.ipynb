{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07434a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/by/kxt996v16zj_zmc7tssff7km0000gn/T/ipykernel_35650/3059967421.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ea0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"complaints_25Nov21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a522dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b904dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Product', 'Sub-product', 'Issue', 'State', 'Tags', 'Submitted via', 'Company response to consumer', 'Timely response?']]\n",
    "\n",
    "# Encoding the 'Consumer disputed?' column as the y-variable\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df['Consumer disputed?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6108db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test sets with an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd4cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/258.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.24.4)\n",
      "Installing collected packages: joblib, imbalanced-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "Successfully installed imbalanced-learn-0.12.2 joblib-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20d08a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9e5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of disputed complaints in the training set: 21.68%\n",
      "Random undersampling applied to balance the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the proportion of disputed complaints in the training set\n",
    "disputed_proportion = np.mean(y_train)\n",
    "\n",
    "print(f\"Proportion of disputed complaints in the training set: {disputed_proportion * 100:.2f}%\")\n",
    "\n",
    "# Check if the proportion is less than 30%\n",
    "if disputed_proportion < 0.30:\n",
    "    # Apply random undersampling to balance the dataset\n",
    "    undersampler = RandomUnderSampler(random_state=123)\n",
    "    X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "    print(\"Random undersampling applied to balance the dataset.\")\n",
    "else:\n",
    "    print(\"The dataset is balanced enough. No need for undersampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c7ac975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: numpy in /Users/trentyu/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.24.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26f659bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=123, ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_test_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "# Ensuring both sets have the same columns, in the same order\n",
    "X_test_encoded = X_test_encoded.reindex(columns = X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "model_xgb = XGBClassifier(random_state=123)\n",
    "model_xgb.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a6c5961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=123, ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(random_state=123)\n",
    "model_xgb.fit(X_train_encoded, y_train)  # Ensure this step is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d7d30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_xgb.predict(X_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89350948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.53      0.65     32504\n",
      "           1       0.27      0.63      0.38      8948\n",
      "\n",
      "    accuracy                           0.55     41452\n",
      "   macro avg       0.55      0.58      0.51     41452\n",
      "weighted avg       0.71      0.55      0.59     41452\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17157 15347]\n",
      " [ 3323  5625]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9ed7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of complaints in the test set\n",
    "total_complaints = len(y_test)\n",
    "\n",
    "# Cost calculation\n",
    "# $100 for every complaint plus $90 for due diligence for each\n",
    "base_case_cost = total_complaints * (100 + 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48823164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since y_test is encoded, we assume 1 represents disputed and 0 represents non-disputed.\n",
    "# Number of actual disputed complaints\n",
    "disputed_complaints = sum(y_test)\n",
    "\n",
    "# Number of non-disputed complaints\n",
    "non_disputed_complaints = total_complaints - disputed_complaints\n",
    "\n",
    "# Actual cost considering real outcomes\n",
    "actual_cost = (disputed_complaints * 600) + (non_disputed_complaints * (100 + 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26ba8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7875880"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_case_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad84693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11544560"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4262c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest total cost: $7612170\n",
      "Best threshold: 0.41000000000000003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_probs = model_xgb.predict_proba(X_test_encoded)[:, 1]  # Probabilities of the positive class\n",
    "\n",
    "# Initialize variables to store the results\n",
    "lowest_cost = float('inf')\n",
    "best_threshold = 0.0\n",
    "\n",
    "# Iterate over a range of thresholds to find the one with the lowest cost\n",
    "for threshold in np.linspace(0, 1, 101):\n",
    "    # Apply threshold to positive class probabilities to create predicted classes\n",
    "    y_pred_threshold = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n",
    "    \n",
    "    # Calculate the total cost based on the provided cost structure\n",
    "    cost = tn * 100 + fp * 190 + fn * 600 + tp * 190\n",
    "    \n",
    "    # Check if this threshold has a lower cost than what we've seen so far\n",
    "    if cost < lowest_cost:\n",
    "        lowest_cost = cost\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Output the lowest cost and the best threshold\n",
    "print(f\"Lowest total cost: ${lowest_cost}\")\n",
    "print(f\"Best threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "163a1e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21586413200810575"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the test set (not the entire dataset), what proportion of consumers raised a dispute?\n",
    "\n",
    "\n",
    "# Calculate the number of disputes\n",
    "number_of_disputes = sum(y_test)\n",
    "\n",
    "# Calculate the total number of instances in the test set\n",
    "total_test_instances = len(y_test)\n",
    "\n",
    "# Calculate the proportion of disputes\n",
    "proportion_of_disputes = number_of_disputes / total_test_instances\n",
    "\n",
    "proportion_of_disputes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98bd7edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After you have performed random undersampling, what proportion of consumers in the training dataset raised a dispute?\n",
    "\n",
    "\n",
    "\n",
    "# Since y_train is the target after random undersampling, we can directly calculate the proportion\n",
    "number_of_disputed = sum(y_train)\n",
    "total_train_instances = len(y_train)\n",
    "\n",
    "# Calculate the proportion of disputes\n",
    "proportion_of_disputed = number_of_disputed / total_train_instances\n",
    "\n",
    "proportion_of_disputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "681e942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the XGBClassifier model as described in the instructions, and evaluate it on the test set.  What is the recall for the category 'Consumer disputed?' = 'Yes' on the test set?\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Assuming 'X_train' and 'X_test' contain categorical features, which we'll encode.\n",
    "\n",
    "# Identify categorical columns (replace this with your actual categorical columns)\n",
    "categorical_columns = ['Product', 'Sub-product', 'Issue', 'State', 'Tags', 'Submitted via', 'Company response to consumer', 'Timely response?']\n",
    "\n",
    "# Create the ColumnTransformer with OneHotEncoder for categorical features\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # This will pass through all other columns without transformation\n",
    ")\n",
    "\n",
    "# Fit the transformer on the training data and transform both training and test data\n",
    "X_train_encoded = column_transformer.fit_transform(X_train)\n",
    "X_test_encoded = column_transformer.transform(X_test)\n",
    "\n",
    "# Now fit the XGBoost model on the encoded training data\n",
    "model_xgb = XGBClassifier(random_state=123)\n",
    "model_xgb.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the encoded test set\n",
    "y_pred = model_xgb.predict(X_test_encoded)\n",
    "\n",
    "# Calculate the recall from the classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "recall_disputed = report['1']['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feb56794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 'Consumer disputed?' = 'Yes': 0.63\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming categorical variables are encoded in X_train and X_test\n",
    "\n",
    "# Initialize the classifier with the random state for reproducibility\n",
    "model_xgb = XGBClassifier(random_state=123)\n",
    "\n",
    "# Fit the model with the training data\n",
    "model_xgb.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model_xgb.predict(X_test_encoded)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# The recall for the 'Consumer disputed?' = 'Yes' category\n",
    "recall_disputed = report['1']['recall']\n",
    "print(f\"Recall for 'Consumer disputed?' = 'Yes': {recall_disputed:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23446d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the XGBClassifier model as described in the instructions, and evaluate it on the test set.  What is the recall for the category 'Consumer disputed?' = 'Yes' on the test set?\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `y_test` contains the actual labels for the test set\n",
    "total_complaints = len(y_test)\n",
    "base_case_cost_per_complaint = 7875880\n",
    "\n",
    "# Total base-case cost\n",
    "total_base_case_cost = total_complaints * base_case_cost_per_complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f6631fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326470977760"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_base_case_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aeff1d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of complaints in the test set is: 41452\n"
     ]
    }
   ],
   "source": [
    "# The number of complaints in the test set is the length of y_test\n",
    "number_of_complaints = len(y_test)\n",
    "\n",
    "# Display the number of complaints\n",
    "print(f\"The total number of complaints in the test set is: {number_of_complaints}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fac74513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24871200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_complaints = 41452\n",
    "cost_per_complaint = 600  # Since without the model we prepare for the worst case for each complaint\n",
    "\n",
    "total_base_case_cost = total_complaints * cost_per_complaint\n",
    "total_base_case_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60276826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the predictions for which complaints are likely to be disputed from the model \n",
    "#you have created (using the default classification threshold).  \n",
    "#Assume that if the model predicts a complaint will be disputed, \n",
    "#the banks decide to spend $90 performing extra diligence to avoid the $600 cost of a dispute.\n",
    "\n",
    "#In this situation based on model results, what would be the total cost to the banks of dealing with the \n",
    "#complaints in the test set?\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_pred contains the model predictions for the test set\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = (tn * 100) + (fp * 190) + (tp * 190) + (fn * 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7bd3b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7688180"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff2ed81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum total cost: $7609520\n",
      "Best threshold: 0.44\n"
     ]
    }
   ],
   "source": [
    "#The costs to the banks from doing due diligence and from having disputes are asymmetrical.  \n",
    "#Therefore you have the opportunity to reduce total cost by varying the probability threshold from the default 0.5 \n",
    "#in a binary classification situation as this.\n",
    "\n",
    "#Change the value of the threshold and determine the lowest total cost to the banks based on the observations \n",
    "#in the test set.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predicted probabilities for the positive class\n",
    "y_probs = model_xgb.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Initialize variables to store the best threshold and minimum cost\n",
    "min_cost = float('inf')\n",
    "best_threshold = 0.0\n",
    "\n",
    "#At what value of the threshold is the lowest dollar cost achieved?\n",
    "\n",
    "\n",
    "# Iterate over a range of threshold values\n",
    "for threshold in np.linspace(0, 1, 101):\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    y_pred_threshold = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n",
    "    \n",
    "    # Calculate the total cost for the current threshold\n",
    "    total_cost = (tn * 100) + (fp * 190) + (tp * 190) + (fn * 600)\n",
    "    \n",
    "    # Update minimum cost and best threshold if current cost is lower\n",
    "    if total_cost < min_cost:\n",
    "        min_cost = total_cost\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Output the minimum cost and the best threshold\n",
    "print(f\"Minimum total cost: ${min_cost}\")\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87bed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
